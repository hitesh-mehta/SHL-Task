{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy pandas librosa torch matplotlib seaborn scikit-learn scipy transformers openai-whisper spacy torchaudio\n!python -m spacy download en_core_web_sm\n!sudo apt-get install -y ffmpeg\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T11:34:27.194280Z","iopub.execute_input":"2025-04-06T11:34:27.194517Z","iopub.status.idle":"2025-04-06T11:35:00.206684Z","shell.execute_reply.started":"2025-04-06T11:34:27.194492Z","shell.execute_reply":"2025-04-06T11:35:00.205569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoTokenizer, AutoModel\nimport whisper\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport spacy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T11:35:22.394876Z","iopub.execute_input":"2025-04-06T11:35:22.395163Z","iopub.status.idle":"2025-04-06T11:35:22.400163Z","shell.execute_reply.started":"2025-04-06T11:35:22.395134Z","shell.execute_reply":"2025-04-06T11:35:22.399330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport librosa\nimport pandas as pd\nimport spacy\n\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    Wav2Vec2Processor,\n    Wav2Vec2ForCTC,\n    AutoTokenizer,\n    AutoModel\n)\nimport whisper\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Set random seed\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nclass AudioTextDataset(Dataset):\n    def __init__(self, csv_file, audio_dir, transform=None, max_audio_length=48000):\n        self.data_frame = pd.read_csv(csv_file)\n        self.audio_dir = audio_dir\n        self.transform = transform\n        self.max_audio_length = max_audio_length\n\n        self.audio_data = []\n        self.transcripts = []\n        self.labels = []\n\n        self.asr_model = whisper.load_model(\"base\")\n        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n        self.wav2vec_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n        self.bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n        for idx, row in self.data_frame.iterrows():\n            print(f\"Processing file {idx+1}/{len(self.data_frame)}: {row['filename']}\")\n            audio_path = os.path.join(self.audio_dir, row['filename'])\n\n            audio, sr = librosa.load(audio_path, sr=16000)\n            audio, _ = librosa.effects.trim(audio, top_db=20)\n\n            if len(audio) > self.max_audio_length:\n                audio = audio[:self.max_audio_length]\n            else:\n                padding = self.max_audio_length - len(audio)\n                audio = np.pad(audio, (0, padding), 'constant')\n\n            audio_features = self._extract_audio_features(audio)\n            transcript = self._transcribe_audio(audio)\n            label = row['label'] if 'label' in row else 0\n\n            self.audio_data.append(audio_features)\n            self.transcripts.append(transcript)\n            self.labels.append(label)\n\n    def _extract_audio_features(self, audio):\n        mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=13)\n        mfcc_delta = librosa.feature.delta(mfccs)\n        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n\n        pitches, magnitudes = librosa.piptrack(y=audio, sr=16000)\n        pitch = np.array([pitches[magnitudes[:, i].argmax(), i] for i in range(magnitudes.shape[1])], dtype=np.float32)\n\n        energy = float(np.mean(librosa.feature.rms(y=audio)))\n\n        with torch.no_grad():\n            inputs = self.wav2vec_processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n            outputs = self.wav2vec_model(**inputs)\n            wav2vec_embedding = outputs.logits.mean(dim=1).squeeze().numpy().astype(np.float32)\n\n        return {\n            'mfccs': mfccs.astype(np.float32),\n            'mfcc_delta': mfcc_delta.astype(np.float32),\n            'mfcc_delta2': mfcc_delta2.astype(np.float32),\n            'pitch': pitch,\n            'energy': energy,\n            'wav2vec_embedding': wav2vec_embedding\n        }\n\n    def _transcribe_audio(self, audio):\n        return self.asr_model.transcribe(audio)[\"text\"]\n\n    def _extract_linguistic_features(self, text):\n        doc = nlp(text)\n\n        num_tokens = len(doc)\n        num_sentences = len(list(doc.sents))\n        avg_token_length = np.mean([len(token.text) for token in doc]) if num_tokens > 0 else 0\n\n        unique_tokens = len(set([token.text.lower() for token in doc]))\n        ttr = unique_tokens / num_tokens if num_tokens > 0 else 0\n\n        pos_counts = {}\n        for token in doc:\n            pos = token.pos_\n            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n\n        for pos in pos_counts:\n            pos_counts[pos] = pos_counts[pos] / num_tokens if num_tokens > 0 else 0\n\n        avg_dependency_distance = np.mean(\n            [abs(token.i - token.head.i) for token in doc if token.head is not token]\n        ) if num_tokens > 0 else 0\n\n        inputs = self.bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        with torch.no_grad():\n            outputs = self.bert_model(**inputs)\n            bert_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy().astype(np.float32)\n\n        return {\n            'num_tokens': float(num_tokens),\n            'num_sentences': float(num_sentences),\n            'avg_token_length': float(avg_token_length),\n            'ttr': float(ttr),\n            'pos_counts': pos_counts,\n            'avg_dependency_distance': float(avg_dependency_distance),\n            'bert_embedding': bert_embedding\n        }\n\n    def __len__(self):\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        audio_features = self.audio_data[idx]\n        transcript = self.transcripts[idx]\n        label = self.labels[idx]\n\n        raw_features = self._extract_linguistic_features(transcript)\n\n        linguistic_features = {\n            'bert_embedding': raw_features['bert_embedding'],\n            'num_tokens': raw_features['num_tokens'],\n            'num_sentences': raw_features['num_sentences'],\n            'avg_token_length': raw_features['avg_token_length'],\n            'ttr': raw_features['ttr'],\n            'avg_dependency_distance': raw_features['avg_dependency_distance'],\n            'pos_counts': raw_features['pos_counts']\n        }\n\n        sample = {\n            'audio_features': audio_features,\n            'linguistic_features': linguistic_features,\n            'transcript': transcript,\n            'grammar_score': label\n        }\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:24:17.646197Z","iopub.execute_input":"2025-04-06T12:24:17.646574Z","iopub.status.idle":"2025-04-06T12:24:18.266200Z","shell.execute_reply.started":"2025-04-06T12:24:17.646544Z","shell.execute_reply":"2025-04-06T12:24:18.265452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Feature Normalization and Preparation\nclass FeatureNormalizer(object):\n    \"\"\"Normalize features to have zero mean and unit variance\"\"\"\n    def __init__(self, feature_means=None, feature_stds=None):\n        self.feature_means = feature_means\n        self.feature_stds = feature_stds\n        \n    def fit(self, dataset):\n        \"\"\"Compute means and stds from dataset\"\"\"\n        # This is a simplified example, you would need to calculate stats\n        # for all numeric features in your actual implementation\n        pass\n        \n    def __call__(self, sample):\n        \"\"\"Normalize features in the sample\"\"\"\n        # This is a simplified example, you would normalize all features\n        # in your actual implementation\n        return sample\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:24:23.756355Z","iopub.execute_input":"2025-04-06T12:24:23.756645Z","iopub.status.idle":"2025-04-06T12:24:23.761420Z","shell.execute_reply.started":"2025-04-06T12:24:23.756621Z","shell.execute_reply":"2025-04-06T12:24:23.760453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Main Pipeline\n\n# Paths and parameters\ntrain_csv = '/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv'\ntest_csv = '/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv'\nsample_submission_csv = '/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv'\ntrain_audio_dir = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train'\ntest_audio_dir = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test'\nbatch_size = 16\nnum_epochs = 25\nlearning_rate = 0.001\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Data preparation\nprint(\"Preparing training data...\")\ntrain_dataset = AudioTextDataset(train_csv, train_audio_dir)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:24:26.377155Z","iopub.execute_input":"2025-04-06T12:24:26.377478Z","iopub.status.idle":"2025-04-06T12:32:26.357159Z","shell.execute_reply.started":"2025-04-06T12:24:26.377446Z","shell.execute_reply":"2025-04-06T12:32:26.356459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Multimodal Model Definition\nclass MultimodalGrammarScorer(nn.Module):\n    def __init__(self, audio_dim=32, text_dim=768, hidden_dim=256):\n        super(MultimodalGrammarScorer, self).__init__()\n        \n        # Audio processing branch\n        self.audio_encoder = nn.Sequential(\n            nn.Linear(audio_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Text processing branch\n        self.text_encoder = nn.Sequential(\n            nn.Linear(text_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Fusion and output layers\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim // 4, 1)\n        )\n        \n    def forward(self, audio_features, text_features):\n        # Process audio branch\n        audio_encoding = self.audio_encoder(audio_features)\n        \n        # Process text branch\n        text_encoding = self.text_encoder(text_features)\n        \n        # Concatenate features\n        combined = torch.cat((audio_encoding, text_encoding), dim=1)\n        \n        # Final prediction\n        output = self.fusion(combined)\n        \n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:42:58.157368Z","iopub.execute_input":"2025-04-06T12:42:58.157680Z","iopub.status.idle":"2025-04-06T12:42:58.164464Z","shell.execute_reply.started":"2025-04-06T12:42:58.157657Z","shell.execute_reply":"2025-04-06T12:42:58.163532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n    \"\"\"Train the model\"\"\"\n    model.to(device)\n    best_val_corr = -1.0\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        for batch in train_loader:\n            # Extract inputs from dict-like batch\n            audio_embeddings = torch.tensor(np.stack(batch['audio_features']['wav2vec_embedding'])).float().to(device)\n            text_embeddings = torch.tensor(np.stack(batch['linguistic_features']['bert_embedding'])).float().to(device)\n            scores = torch.tensor(batch['grammar_score']).float().to(device)\n\n            # Forward and backward pass\n            optimizer.zero_grad()\n            outputs = model(audio_embeddings, text_embeddings)\n            loss = criterion(outputs.squeeze(), scores)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        epoch_loss = running_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        val_predictions = []\n        val_targets = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                audio_embeddings = torch.tensor(np.stack(batch['audio_features']['wav2vec_embedding'])).float().to(device)\n                text_embeddings = torch.tensor(np.stack(batch['linguistic_features']['bert_embedding'])).float().to(device)\n                scores = torch.tensor(batch['grammar_score']).float().to(device)\n\n                outputs = model(audio_embeddings, text_embeddings)\n                predictions = outputs.squeeze().cpu().numpy()\n                val_predictions.extend(predictions)\n                val_targets.extend(scores.cpu().numpy())\n\n        val_corr, _ = pearsonr(val_targets, val_predictions)\n        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Val Corr: {val_corr:.4f} - Val RMSE: {val_rmse:.4f}\")\n\n        if val_corr > best_val_corr:\n            best_val_corr = val_corr\n            torch.save(model.state_dict(), 'best_grammar_model.pt')\n\n    return model\n\n\ndef evaluate_model(model, test_loader, device='cuda'):\n    \"\"\"Evaluate the model on test data\"\"\"\n    model.to(device)\n    model.eval()\n\n    predictions = []\n    targets = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_embeddings = torch.tensor(np.stack(batch['audio_features']['wav2vec_embedding'])).float().to(device)\n            text_embeddings = torch.tensor(np.stack(batch['linguistic_features']['bert_embedding'])).float().to(device)\n            scores = torch.tensor(batch['grammar_score']).float().to(device)\n\n            outputs = model(audio_embeddings, text_embeddings)\n            batch_predictions = outputs.squeeze().cpu().numpy()\n\n            predictions.extend(batch_predictions)\n            targets.extend(scores.cpu().numpy())\n\n    if np.any(targets):\n        corr, p_value = pearsonr(targets, predictions)\n        rmse = np.sqrt(mean_squared_error(targets, predictions))\n        mae = mean_absolute_error(targets, predictions)\n\n        print(f\"Test Results - Pearson Correlation: {corr:.4f} (p={p_value:.4f}), RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\n        plt.figure(figsize=(10, 6))\n        plt.scatter(targets, predictions, alpha=0.6)\n        plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')\n        plt.xlabel('Actual Grammar Scores')\n        plt.ylabel('Predicted Grammar Scores')\n        plt.title(f'Actual vs Predicted Grammar Scores (Pearson r={corr:.4f})')\n        plt.grid(True)\n        plt.savefig('grammar_score_predictions.png')\n\n        return predictions, {'correlation': corr, 'rmse': rmse, 'mae': mae}\n    else:\n        return predictions, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:42:59.205223Z","iopub.execute_input":"2025-04-06T12:42:59.205507Z","iopub.status.idle":"2025-04-06T12:42:59.218039Z","shell.execute_reply.started":"2025-04-06T12:42:59.205484Z","shell.execute_reply":"2025-04-06T12:42:59.217156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data._utils.collate import default_collate\n\n# Cross-validation setup\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfold_results = []\n\ndef custom_collate(batch):\n    required_top_keys = {'audio_features', 'linguistic_features', 'grammar_score'}\n    required_linguistic_keys = {'bert_embedding'}  # Keep only the BERT embedding\n\n    filtered_batch = []\n    for item in batch:\n        filtered_item = {}\n\n        for k in required_top_keys:\n            if k not in item:\n                continue\n\n            if k == 'linguistic_features':\n                filtered_item[k] = {sub_k: sub_v for sub_k, sub_v in item[k].items() if sub_k in required_linguistic_keys}\n            else:\n                filtered_item[k] = item[k]\n\n        filtered_batch.append(filtered_item)\n\n    return default_collate(filtered_batch)\n\n\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n    print(f\"\\nTraining fold {fold+1}/5...\")\n\n    # Split dataset\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n\n    # DataLoaders with custom collate_fn\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler, collate_fn=custom_collate)\n    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_subsampler, collate_fn=custom_collate)\n\n    # Initialize model\n    model = MultimodalGrammarScorer()\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n    # Train model\n    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n\n    # Evaluate on validation set\n    val_predictions, val_metrics = evaluate_model(model, val_loader, device)\n    fold_results.append(val_metrics)\n\n# Summarize cross-validation results\nprint(\"\\nCross-validation results:\")\nfor i, result in enumerate(fold_results):\n    print(f\"Fold {i+1}: Correlation = {result['correlation']:.4f}, RMSE = {result['rmse']:.4f}\")\n\navg_corr = np.mean([r['correlation'] for r in fold_results])\navg_rmse = np.mean([r['rmse'] for r in fold_results])\nprint(f\"Average: Correlation = {avg_corr:.4f}, RMSE = {avg_rmse:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:43:01.801282Z","iopub.execute_input":"2025-04-06T12:43:01.801585Z","iopub.status.idle":"2025-04-06T13:36:09.480793Z","shell.execute_reply.started":"2025-04-06T12:43:01.801560Z","shell.execute_reply":"2025-04-06T13:36:09.479780Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final model training on full dataset\nprint(\"\\nTraining final model on full dataset...\")\n\n# Important: Use the same collate function here as well\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n\nfinal_model = MultimodalGrammarScorer()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(final_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n# You can use val_loader = train_loader just for consistency, or use a small split if needed\nfinal_model = train_model(final_model, train_loader, train_loader, criterion, optimizer, num_epochs, device)\n\n# Prepare test data\nprint(\"\\nPreparing test data...\")\ntest_dataset = AudioTextDataset(test_csv, test_audio_dir)\n\n# Again: use the collate function here too\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n\n# Generate predictions for test set\ntest_predictions, _ = evaluate_model(final_model, test_loader, device)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:40:06.939124Z","iopub.execute_input":"2025-04-06T13:40:06.939425Z","iopub.status.idle":"2025-04-06T14:04:20.573609Z","shell.execute_reply.started":"2025-04-06T13:40:06.939403Z","shell.execute_reply":"2025-04-06T14:04:20.572650Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.read_csv(sample_submission_csv)\nsubmission['label'] = test_predictions\n\n# Clip predictions to valid range (0 to 5)\nsubmission['label'] = submission['label'].clip(0, 5)\n\n# Round to nearest 0.5\nsubmission['label'] = (submission['label'] * 2).round() / 2\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"✅ Submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:04:44.941407Z","iopub.execute_input":"2025-04-06T14:04:44.941801Z","iopub.status.idle":"2025-04-06T14:04:44.964185Z","shell.execute_reply.started":"2025-04-06T14:04:44.941771Z","shell.execute_reply":"2025-04-06T14:04:44.963465Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}